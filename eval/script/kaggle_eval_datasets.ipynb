{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5017a5a",
   "metadata": {},
   "source": [
    "### 1. Kaggle Evaluation Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd440d18",
   "metadata": {},
   "source": [
    "#### 1.1 Unzip datasets in each folder\n",
    "- We downloaded datasets using Kaggle's API, resulting in 5,221 folders. Each folder contains a `dataset-metadata.json` file with metadata in the following structure:\n",
    "  ```\n",
    "{\n",
    "  \"id\":\n",
    "  \"id_no\":\n",
    "  \"datasetSlugNullable\":\n",
    "  \"ownerUserNullable\":\n",
    "  \"usabilityRatingNullable\":\n",
    "  \"titleNullable\":\n",
    "  \"subtitleNullable\":\n",
    "  \"descriptionNullable\":\n",
    "  \"datasetId\":\n",
    "  \"datasetSlug\":\n",
    "  \"hasDatasetSlug\":\n",
    "  \"ownerUser\":\n",
    "  \"hasOwnerUser\":\n",
    "  \"usabilityRating\":\n",
    "  \"hasUsabilityRating\":\n",
    "  \"totalViews\":\n",
    "  \"totalVotes\":\n",
    "  \"totalDownloads\":\n",
    "  \"title\":\n",
    "  \"hasTitle\":\n",
    "  \"subtitle\":\n",
    "  \"hasSubtitle\":\n",
    "  \"description\":\n",
    "  \"hasDescription\":\n",
    "  \"isPrivate\":\n",
    "  \"keywords\":\n",
    "  \"licenses\":\n",
    "  \"collaborators\":\n",
    "  \"data\":\n",
    "}\n",
    "```\n",
    "- Each folder also contains a zip file with the corresponding datasets. Our first step is to iterate through all 5,221 folders and unzip all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba19156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the folders with zip files\n",
    "base_directory = os.path.join(os.getcwd(), \"kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over all folders in the base directory\n",
    "# for folder_name in os.listdir(base_directory):\n",
    "#     folder_path = os.path.join(base_directory, folder_name)\n",
    "    \n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Look for zip files in the directory\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             if file_name.endswith(\".zip\"):\n",
    "#                 zip_path = os.path.join(folder_path, file_name)\n",
    "                \n",
    "#                 # Try to unzip the file\n",
    "#                 try:\n",
    "#                     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#                         zip_ref.extractall(folder_path)\n",
    "#                     print(f\"Unzipped: {zip_path}\")\n",
    "#                 except zipfile.BadZipFile as e:\n",
    "#                     print(f\"Failed to unzip {zip_path}: {e}\")\n",
    "\n",
    "# print(\"All zip files have been unzipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a43d4",
   "metadata": {},
   "source": [
    "#### 1.2 Check the files under each folder downloaded from Kaggle\n",
    "\n",
    "- Original unfiltered # of files in total: 12,533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42db66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71608f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "folder_names = []  # list of names of folders containing each dataset\n",
    "dataset_names = []  # list of \"title\" fields from dataset-metadata.json\n",
    "file_names = []  # list of all files in each folder except for dataset-metadata.json and zip files\n",
    "licenses = []  # list of \"licenses name\" fields from dataset-metadata.json\n",
    "descriptions = []  # list of \"description\" fields from dataset-metadata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b939835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each Kaggle dataset folder\n",
    "def process_dataset_folder(folder_path):\n",
    "    try:\n",
    "        # Path to the dataset-metadata.json file\n",
    "        metadata_path = os.path.join(folder_path, 'dataset-metadata.json')\n",
    "        \n",
    "        # Read the dataset-metadata.json file\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Extract the required information\n",
    "        dataset_name = metadata.get('title', '')\n",
    "        license_name = metadata.get('licenses', [{}])[0].get('name', '')\n",
    "        description = metadata.get('description', '')\n",
    "\n",
    "        # List all files in the folder except for dataset-metadata.json and zip files\n",
    "        files = [f for f in os.listdir(folder_path) if f != 'dataset-metadata.json' and not f.endswith('.zip')]\n",
    "\n",
    "        # Store the information in the lists\n",
    "        for file in files:\n",
    "            folder_names.append(os.path.basename(folder_path))\n",
    "            dataset_names.append(dataset_name)\n",
    "            file_names.append(file)\n",
    "            licenses.append(license_name)\n",
    "            descriptions.append(description)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each folder in the base directory\n",
    "for folder_name in tqdm(os.listdir(base_directory), desc=\"Processing Kaggle Datasets\"):\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_dataset_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579800a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the DataFrame\n",
    "data = {\n",
    "    'Folder Name': folder_names,\n",
    "    'Dataset Name': dataset_names,\n",
    "    'File Name': file_names,\n",
    "    'License': licenses,\n",
    "    'Description': descriptions\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Adjust display options to show the complete DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168749e",
   "metadata": {},
   "source": [
    "#### 1.3 Datasets pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de169c",
   "metadata": {},
   "source": [
    "##### 1.3.1 Filter out csv files\n",
    "- 12,533 -> 8,629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc93fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract file types (extensions) and make them lowercase\n",
    "df['File Type'] = df['File Name'].apply(lambda x: os.path.splitext(x)[1].lower())\n",
    "\n",
    "# Analyze the file types\n",
    "file_type_counts = df['File Type'].value_counts().reset_index()\n",
    "file_type_counts.columns = ['File Type', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original df to include only CSV files\n",
    "csv_df = df[df['File Type'] == '.csv']\n",
    "\n",
    "# Reset the index of the filtered DataFrame\n",
    "csv_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b12e8",
   "metadata": {},
   "source": [
    "##### 1.3.2 Filter out datasets w/ allowed licenses\n",
    "- 8,629 -> 7,012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the license\n",
    "license_counts = csv_df['License'].value_counts().reset_index()\n",
    "license_counts.columns = ['License', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44642edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the csv_df for allowed licenses\n",
    "not_allowed_licenses = [\n",
    "    \"unknown\",\n",
    "    \"copyright-authors\",\n",
    "]\n",
    "\n",
    "csv_df_licensed = csv_df[~csv_df['License'].isin(not_allowed_licenses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b739b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df_licensed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac70dee",
   "metadata": {},
   "source": [
    "##### 1.3.3 Filter out datasets w/ description\n",
    "- 7,012 -> 6,520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out datasets with descriptions\n",
    "csv_df_desc = csv_df_licensed[csv_df_licensed['Description'].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c151ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_df_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cacfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(csv_df_desc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af684d",
   "metadata": {},
   "source": [
    "##### 1.3.4 Check dataset-table mapping\n",
    "- 6,520 -> 2,357 (one-to-one mapping only: each dataset contains ONLY one table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Dataset Name and count the number of File Names for each Dataset\n",
    "table_count_per_dataset = csv_df_desc.groupby('Dataset Name').size().reset_index(name='Table Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_count_per_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets that have only one table\n",
    "one_table_per_dataset = table_count_per_dataset[table_count_per_dataset['Table Count'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef53462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(one_table_per_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with the original dataframe `csv_df_desc` to get all other attributes\n",
    "one_table_datasets = pd.merge(one_table_per_dataset, csv_df_desc, on='Dataset Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(one_table_datasets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfa98a",
   "metadata": {},
   "source": [
    "- 6,520 -> 4,163 (one-to-multiple mapping: each dataset contains MULTIPLE tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter datasets that have multiple tables\n",
    "multi_table_per_dataset = table_count_per_dataset[table_count_per_dataset['Table Count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3305bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_per_dataset['Table Count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with the original dataframe `csv_df_desc` to get all other attributes\n",
    "multi_table_datasets = pd.merge(multi_table_per_dataset, csv_df_desc, on='Dataset Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_table_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4926a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chardet\n",
    "\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(10000)  # Read only the first 10k bytes\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_multiple_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252']\n",
    "    detected_encoding = detect_encoding(file_path)\n",
    "    encodings.insert(0, detected_encoding)  # Try detected encoding first\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(file_path, nrows=0, encoding=encoding)  # Read only the header\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return None  # If all attempts fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_schema_consistency(folder_name, base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if not csv_files:\n",
    "        return False\n",
    "\n",
    "    schemas = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            df = read_csv_with_multiple_encodings(file_path)\n",
    "            if df is not None:\n",
    "                schemas.append(set(df.columns))\n",
    "            else:\n",
    "                print(f\"Error reading {file_path}: Unable to decode with common encodings\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Check if all schemas are identical\n",
    "    first_schema = schemas[0]\n",
    "    for schema in schemas:\n",
    "        if schema != first_schema:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9acb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column \"Schema Consistency\" to `multi_table_datasets`\n",
    "schema_consistency = []\n",
    "\n",
    "for folder_name in tqdm(multi_table_datasets['Folder Name'].unique(), desc=\"Checking Schema Consistency\"):\n",
    "    consistency = check_schema_consistency(folder_name, base_directory)\n",
    "    schema_consistency.append((folder_name, consistency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the schema_consistency list to a DataFrame\n",
    "schema_consistency_df = pd.DataFrame(schema_consistency, columns=['Folder Name', 'Schema Consistency'])\n",
    "\n",
    "# Join the schema consistency results with `multi_table_datasets`\n",
    "multi_table_datasets = pd.merge(multi_table_datasets, schema_consistency_df, on='Folder Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_datasets['Schema Consistency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_table_same_schema = multi_table_datasets[multi_table_datasets['Schema Consistency'] == True]\n",
    "multi_table_diff_schema = multi_table_datasets[multi_table_datasets['Schema Consistency'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets to CSV files\n",
    "one_table_datasets.to_csv('one_table_datasets.csv', index=False)\n",
    "multi_table_same_schema.to_csv('multi_table_same_schema.csv', index=False)\n",
    "multi_table_diff_schema.to_csv('multi_table_diff_schema.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4993ec",
   "metadata": {},
   "source": [
    "#### 1.4 Visualize # of datasets after different stages of filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the stacked bar chart\n",
    "categories = ['One-to-One Mapping', 'One-to-Multiple Mapping']\n",
    "values_one_to_one = [2357, 0]\n",
    "values_multiple_consistent = [0, 1461]\n",
    "values_multiple_inconsistent = [0, 2702]\n",
    "\n",
    "# Data for the funnel chart\n",
    "stages = [\n",
    "    \"Total Datasets from Kaggle\",  # df\n",
    "    \"Filter by CSV\",  # csv_df\n",
    "    \"Filter by Licensing\",  # csv_df_licensed\n",
    "    \"Filter by Descriptions\",  # csv_df_desc\n",
    "]\n",
    "counts = [12533, 8629, 7012, 6520]\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plotting the funnel chart on the first subplot\n",
    "ax1.barh(stages, counts)\n",
    "\n",
    "# Adding labels on the bars for the funnel chart\n",
    "for index, value in enumerate(counts):\n",
    "    ax1.text(value, index, str(value), va='center')\n",
    "\n",
    "# Setting title and labels for the first plot\n",
    "ax1.set_title('Funnel Chart of Dataset Filtering Stages')\n",
    "ax1.set_xlabel('Number of Files')\n",
    "ax1.set_ylabel('Filtering Stages')\n",
    "ax1.invert_yaxis()  # Reverse the order of stages for a funnel effect\n",
    "\n",
    "# Plotting the stacked bar chart on the second subplot\n",
    "p1 = ax2.bar(categories, values_one_to_one, label='One-to-One Mapping')\n",
    "p2 = ax2.bar(categories, values_multiple_consistent, bottom=values_one_to_one, label='One-to-Multiple (Consistent Schema)')\n",
    "p3 = ax2.bar(categories, values_multiple_inconsistent, bottom=[i+j for i,j in zip(values_one_to_one, values_multiple_consistent)], label='One-to-Multiple (Inconsistent Schema)')\n",
    "\n",
    "# Add text labels on the bars for the stacked bar chart\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        if yval > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_y() + yval/2, int(yval), ha='center', va='center', color='white')\n",
    "\n",
    "add_labels(p1, ax2)\n",
    "add_labels(p2, ax2)\n",
    "add_labels(p3, ax2)\n",
    "\n",
    "# Setting title and labels for the second plot\n",
    "ax2.set_title('Dataset-Table Mapping')\n",
    "ax2.set_xlabel('Mapping Type')\n",
    "ax2.set_ylabel('Number of Tables')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3e216",
   "metadata": {},
   "source": [
    "### 2. Datasets Metadata Enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb330b",
   "metadata": {},
   "source": [
    "#### 2.1 Generate keywords & tasks for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5363125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into dataframe\n",
    "one_table_datasets = pd.read_csv('one_table_datasets.csv')\n",
    "multi_table_same_schema = pd.read_csv('multi_table_same_schema.csv')\n",
    "multi_table_diff_schema = pd.read_csv('multi_table_diff_schema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ed846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key and model name\n",
    "load_dotenv()\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keywords_and_queries(description, dataset_name, filenames=None):\n",
    "    if filenames:\n",
    "        filenames_str = \"\\n- \".join(filenames)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            Given a dataset that contains CSV files with the below file names:\n",
    "                - {filenames_str}\n",
    "            And the description of the dataset provided below:\n",
    "                {description}\n",
    "\n",
    "            Generate a dictionary in JSON format with the CSV file names as keys and as values a list of 4 semantically distinct data content keywords that describe the expected content of each CSV file but do not describe specific analytic tasks possible with the file.\n",
    "            For each CSV file, also add a list of 3 semantically distinct analytics task sentences that can be performed with the described CSV file, e.g. develop ML model to predict XYZ.\n",
    "\n",
    "            Example response format:\n",
    "            {{\n",
    "                \"csv file 1\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }},\n",
    "                \"csv file 2\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "            Based on the dataset description provided below, generate a dictionary in JSON format with the dataset name as key and as values a list of 4 semantically distinct data content keywords that describe the expected content of the dataset but do not describe specific analytic tasks possible with the data. \n",
    "            Also, add a list of 3 semantically distinct analytics task sentences that can be performed with the described dataset, e.g. develop ML model to predict XYZ.\n",
    "            If it is hard to complete the task, return an empty dictionary instead.\n",
    "\n",
    "            Dataset Description:\n",
    "            \"{description}\"\n",
    "\n",
    "            Example response format:\n",
    "            {{\n",
    "                \"{dataset_name}\": {{\n",
    "                    \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\"],\n",
    "                    \"task_queries\": [\"Task query 1\", \"Task query 2\", \"Task query 3\"]\n",
    "                }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed generate keywords and task-based queries for tables.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "    )\n",
    "\n",
    "    keywords_and_queries = response.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        return json.loads(keywords_and_queries)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to decode JSON response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95569038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datasets(one_table_datasets, multi_table_same_schema, multi_table_diff_schema):\n",
    "    results = []\n",
    "\n",
    "    # Process one_table_datasets and multi_table_same_schema\n",
    "    for df, table_type in zip([one_table_datasets, multi_table_same_schema], ['one_table', 'multi_table_same_schema']):\n",
    "        for dataset_name, group in tqdm(df.groupby('Dataset Name'), desc=f\"Processing {table_type} datasets\"):\n",
    "            description = group.iloc[0]['Description']\n",
    "            keywords_and_queries = generate_keywords_and_queries(description, dataset_name)\n",
    "            for _, row in group.iterrows():\n",
    "                file_name = row['File Name']\n",
    "                keywords = keywords_and_queries.get(dataset_name, {}).get('keywords', [])\n",
    "                task_queries = keywords_and_queries.get(dataset_name, {}).get('task_queries', [])\n",
    "                results.append((row['Dataset Name'], file_name, keywords, task_queries, table_type))\n",
    "    \n",
    "    # Process multi_table_diff_schema\n",
    "    for index, row in tqdm(multi_table_diff_schema.iterrows(), desc=\"Processing multi_table_diff_schema datasets\", total=multi_table_diff_schema.shape[0]):\n",
    "        dataset_name = row['Dataset Name']\n",
    "        description = row['Description']\n",
    "        csv_file = row['File Name']\n",
    "        \n",
    "        keywords_and_queries = generate_keywords_and_queries(description, dataset_name, [csv_file])\n",
    "        keywords = keywords_and_queries.get(csv_file, {}).get('keywords', [])\n",
    "        task_queries = keywords_and_queries.get(csv_file, {}).get('task_queries', [])\n",
    "        results.append((dataset_name, csv_file, keywords, task_queries, 'multi_table_diff_schema'))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_samples(one_table_datasets, multi_table_same_schema, multi_table_diff_schema, n=5):\n",
    "    one_table_sample = one_table_datasets.sample(n=min(n, len(one_table_datasets)))\n",
    "    multi_table_same_schema_sample = multi_table_same_schema.sample(n=min(n, len(multi_table_same_schema)))\n",
    "    multi_table_diff_schema_sample = multi_table_diff_schema.sample(n=min(n, len(multi_table_diff_schema)))\n",
    "    \n",
    "    return one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f02f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 20 test samples\n",
    "one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample = create_test_samples(one_table_datasets, multi_table_same_schema, multi_table_diff_schema, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test samples\n",
    "test_results = process_datasets(one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "test_results_df = pd.DataFrame(test_results, columns=['Dataset Name', 'CSV File', 'Keywords', 'Task Queries', 'Table Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db065cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the `test_results_df` back to the corresponding original DataFrames\n",
    "one_table_sample = pd.merge(one_table_sample, test_results_df[test_results_df['Table Type'] == 'one_table'], \n",
    "                            left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')\n",
    "\n",
    "multi_table_same_schema_sample = pd.merge(multi_table_same_schema_sample, test_results_df[test_results_df['Table Type'] == 'multi_table_same_schema'], \n",
    "                                          left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')\n",
    "\n",
    "multi_table_diff_schema_sample = pd.merge(multi_table_diff_schema_sample, test_results_df[test_results_df['Table Type'] == 'multi_table_diff_schema'], \n",
    "                                          left_on=['Dataset Name', 'File Name'], right_on=['Dataset Name', 'CSV File'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the DataFrames\n",
    "union_df_sample = pd.concat([one_table_sample, multi_table_same_schema_sample, multi_table_diff_schema_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all the datasets\n",
    "results = process_datasets(one_table_datasets, multi_table_same_schema, multi_table_diff_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa7021",
   "metadata": {},
   "source": [
    "#### 2.2 Add example rows (in markdown format) to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "import csv\n",
    "import signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed380a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into dataframe\n",
    "one_table_datasets = pd.read_csv('one_table_datasets.csv')\n",
    "multi_table_same_schema = pd.read_csv('multi_table_same_schema.csv')\n",
    "multi_table_diff_schema = pd.read_csv('multi_table_diff_schema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56269eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "# Set the signal handler for the timeout\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Function to detect file encoding\n",
    "def detect_encoding(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Path is not a file: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(10000)  # Read only the first 10k bytes\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']\n",
    "\n",
    "# Function to read CSV with multiple encodings\n",
    "def read_csv_with_multiple_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252', 'utf-16']\n",
    "    detected_encoding = detect_encoding(file_path)\n",
    "    if detected_encoding and detected_encoding.lower() not in encodings and detected_encoding.lower() != 'ascii':\n",
    "        encodings.insert(0, detected_encoding)  # Try detected encoding first\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Set the timeout for reading a file\n",
    "            signal.alarm(5)  # Timeout in seconds\n",
    "            df = pd.read_csv(file_path, encoding=encoding, sep=None, engine='python', on_bad_lines='skip')  # Skip bad lines\n",
    "            signal.alarm(0)  # Reset the alarm\n",
    "            return df\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while reading file '{file_path}' with encoding '{encoding}'\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return None  # If all attempts fail\n",
    "\n",
    "# Function to format example rows into markdown\n",
    "def format_table_markdown(nested_array, n_rows=10):\n",
    "    # The first row of the array is the header\n",
    "    headers = nested_array[0]\n",
    "    # The rest of the array are the data rows\n",
    "    data_rows = nested_array[1:n_rows]\n",
    "\n",
    "    # Start building the Markdown table\n",
    "    markdown = \"| \" + \" | \".join(str(header) for header in headers) + \" |\\n\"\n",
    "    markdown += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "\n",
    "    # Add data rows\n",
    "    for row in data_rows:\n",
    "        markdown += \"| \" + \" | \".join(str(item) for item in row) + \" |\\n\"\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4737a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add example rows to the DataFrames\n",
    "def add_example_rows_to_dataframe(df):\n",
    "    example_rows = []\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding example rows\"):\n",
    "        file_path = os.path.join('kaggle', row['Folder Name'], row['File Name'])\n",
    "        start_time = time.time()\n",
    "        # print(f\"Processing file {idx + 1}/{len(df)}: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            signal.alarm(10)  # Timeout for the entire file processing\n",
    "            df_sample = read_csv_with_multiple_encodings(file_path)\n",
    "            signal.alarm(0)  # Reset the alarm\n",
    "\n",
    "            if df_sample is None:\n",
    "                error_count += 1\n",
    "                print(f\"Failed to read file: {file_path}\")\n",
    "                example_rows.append([])\n",
    "            elif len(df_sample) == 0:\n",
    "                error_count += 1\n",
    "                print(f\"Empty DataFrame for file: {file_path}\")\n",
    "                example_rows.append([])\n",
    "            else:\n",
    "                example = format_table_markdown([df_sample.columns] + df_sample.head(10).values.tolist())\n",
    "                example_rows.append(example)\n",
    "        \n",
    "        except TimeoutException:\n",
    "            error_count += 1\n",
    "            print(f\"Timeout while processing file '{file_path}'\")\n",
    "            example_rows.append([])  # Append an empty list if processing times out\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"Unexpected error processing file '{file_path}': {e}\")\n",
    "            example_rows.append([])  # Append an empty list if there's any other error\n",
    "\n",
    "        end_time = time.time()\n",
    "        # print(f\"Processed file {idx + 1}/{len(df)} in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Explicitly flush the output to ensure logs are written in real-time\n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    df['Example Rows'] = example_rows\n",
    "\n",
    "    print(f\"Total number of tables that encountered errors: {error_count}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example rows in markdown format to the three DataFrames\n",
    "one_table_datasets = add_example_rows_to_dataframe(one_table_datasets)\n",
    "multi_table_same_schema = add_example_rows_to_dataframe(multi_table_same_schema)\n",
    "multi_table_diff_schema = add_example_rows_to_dataframe(multi_table_diff_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c14133",
   "metadata": {},
   "source": [
    "#### 2.3 Generate time & geographic granularity for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_granularities(filename, table_markdown):\n",
    "    prompt = f\"\"\"\n",
    "        Given a table with file name '{filename}' with its header and first few example records: {table_markdown}, determine the most likely geographic or temporal granularity reflected in the dataset.\n",
    "        Select the temporal granularity from the following options: Year, Quarter, Month, Week, Day, Hour, Minute, or Second. For the geographic granularity, choose from: Continent, Country, State/Province, County/District, City, or Zip Code/Postal Code. \n",
    "        Identify the granularities if reflected based on the table provided, or leave empty if it cannot be inferred from the table.\n",
    "\n",
    "        Generate a dictionary in JSON format with two keys: 'time_granularity' and 'geo_granularity'.\n",
    "        Example response format:\n",
    "        {{\n",
    "            \"time_granularity\": time_granularity,\n",
    "            \"geo_granularity\": geo_granularity\n",
    "        }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant designed generate granularities for tables.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        granularities = response.choices[0].message.content \n",
    "        \n",
    "        return json.loads(granularities)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to decode JSON response\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into dataframe\n",
    "one_table_datasets = pd.read_csv('one_table_datasets_with_rows.csv')\n",
    "multi_table_same_schema = pd.read_csv('multi_table_same_schema_with_rows.csv')\n",
    "multi_table_diff_schema = pd.read_csv('multi_table_diff_schema_with_rows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5752f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the DataFrame and generate granularities\n",
    "def process_and_generate_granularities(df, df_name):\n",
    "    results = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), desc=f\"Processing {df_name}\", total=df.shape[0]):\n",
    "        dataset_name = row['Dataset Name']\n",
    "        table_num = row['Table Count']\n",
    "        folder_name = row['Folder Name']\n",
    "        csv_file = row['File Name']\n",
    "        example_rows = row['Example Rows']\n",
    "        description = row['Description']\n",
    "\n",
    "        try:\n",
    "            granularities = generate_granularities(csv_file, example_rows)\n",
    "            time_granularity = granularities.get('time_granularity', '')\n",
    "            geo_granularity = granularities.get('geo_granularity', '')\n",
    "            results.append((dataset_name, table_num, folder_name, csv_file, example_rows, time_granularity, geo_granularity, description, df_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating granularities from file '{csv_file}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create a DataFrame from the results and save it to a CSV file\n",
    "    results_df = pd.DataFrame(results, columns=['Dataset Name', 'Table Count', 'Folder Name', 'File Name', 'Example Rows', 'Time Granularity', 'Geographic Granularity', 'Description', 'Schema Type'])\n",
    "    results_df.to_csv(f'{df_name}_with_granu.csv', index=False)\n",
    "    print(f\"Saved results to {df_name}_with_granu.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_generate_granularities(one_table_datasets, 'one_table_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_generate_granularities(multi_table_same_schema, 'multi_table_same_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e86dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_generate_granularities(multi_table_diff_schema, 'multi_table_diff_schema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into dataframe\n",
    "one_table_datasets_with_granu = pd.read_csv('one_table_datasets_with_granu.csv')\n",
    "multi_table_same_schema_with_granu = pd.read_csv('multi_table_same_schema_with_granu.csv')\n",
    "multi_table_diff_schema_with_granu = pd.read_csv('multi_table_diff_schema_with_granu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea30738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the 3 dataframes\n",
    "granu_df = pd.concat([one_table_datasets_with_granu, multi_table_same_schema_with_granu, multi_table_diff_schema_with_granu], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf95dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_table_datasets: 2,357 -> 2,355\n",
    "# multi_table_same_schema: 1,461 -> 1,461\n",
    "# multi_table_diff_schema: 2,702 -> 2,701\n",
    "table_counts = filtered_granu_df.groupby('Schema Type').size().reset_index(name='Count')\n",
    "table_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208e7a9",
   "metadata": {},
   "source": [
    "#### 2.4 Add # of columns & # of rows to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of columns in the Example Rows column\n",
    "def count_columns(example_rows):\n",
    "    if pd.isna(example_rows) or example_rows == '':\n",
    "        return 0\n",
    "    headers = example_rows.split('\\n')[0].strip('|').split('|')\n",
    "    return len(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df['Number of Columns'] = filtered_granu_df['Example Rows'].apply(count_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9070f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of rows in the original CSV file\n",
    "def count_rows(folder_name, file_name):\n",
    "    file_path = os.path.join('kaggle', folder_name, file_name)\n",
    "    df = read_csv_with_multiple_encodings(file_path)\n",
    "    if df is not None:\n",
    "        return len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Counting rows\")\n",
    "\n",
    "filtered_granu_df['Number of Rows'] = filtered_granu_df.progress_apply(\n",
    "    lambda row: count_rows(row['Folder Name'], row['File Name']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'Example Rows' is empty list or 'Number of Rows' is NA\n",
    "filtered_granu_df = filtered_granu_df[(filtered_granu_df['Example Rows'] != '[]')]\n",
    "filtered_granu_df = filtered_granu_df.dropna(subset=['Number of Rows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Number of Rows' to int\n",
    "filtered_granu_df['Number of Rows'] = filtered_granu_df['Number of Rows'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66487d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df.to_csv('granu_with_column_row_counts_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_granu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cec7e8",
   "metadata": {},
   "source": [
    "#### 2.5 Add popularity (`totalDownloads`), tags (`keywords`) & usability rating (`usabilityRating`) to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a57591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(folder_name):\n",
    "    file_path = os.path.join('kaggle', folder_name, 'dataset-metadata.json')\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "                total_downloads = metadata.get('totalDownloads', None)\n",
    "                keywords = metadata.get('keywords', [])\n",
    "                formatted_keywords = str(keywords) if keywords else \"[]\"\n",
    "                usability_rating = metadata.get('usabilityRating', None)\n",
    "                return total_downloads, formatted_keywords, usability_rating\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading metadata for folder '{folder_name}': {e}\")\n",
    "            return None, \"[]\", None\n",
    "    else:\n",
    "        print(f\"Metadata file not found for folder '{folder_name}'\")\n",
    "        return None, \"[]\", None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df[['Popularity', 'Tags', 'Usability Rating']] = filtered_granu_df.apply(\n",
    "    lambda row: pd.Series(extract_metadata(row['Folder Name'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe3a9b",
   "metadata": {},
   "source": [
    "#### 2.6 Add file size to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bb4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size(folder_name, file_name):\n",
    "    file_path = os.path.join('kaggle', folder_name, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            size = os.path.getsize(file_path)\n",
    "            return size\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting size for file '{file_name}' in folder '{folder_name}': {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df['File Size (bytes)'] = filtered_granu_df.apply(\n",
    "    lambda row: get_file_size(row['Folder Name'], row['File Name']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe84bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_granu_df.to_csv('eval_dataset_with_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_granu_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfad4b",
   "metadata": {},
   "source": [
    "#### 2.7 Add keywords & task queries to each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_queries = pd.read_csv('task_keyword_queries_kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92625db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value in \"Table Type\" column from \"one_table\" to \"one_table_datasets\"\n",
    "task_keyword_queries['Table Type'] = task_keyword_queries['Table Type'].replace('one_table', 'one_table_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_df = pd.merge(\n",
    "    filtered_granu_df,\n",
    "    task_keyword_queries[['Dataset Name', 'File Name', 'Keywords', 'Task Queries', 'Table Type']],\n",
    "    left_on=['Dataset Name', 'File Name', 'Schema Type'],\n",
    "    right_on=['Dataset Name', 'File Name', 'Table Type'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4118912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Table Type column from task_keyword_queries\n",
    "task_keyword_df = task_keyword_df.drop(columns=['Table Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c58b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_df.to_csv('eval_dataset_with_task_keyword.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704332d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6da809",
   "metadata": {},
   "source": [
    "#### 2.8 Generate metadata queries for ALL metadata available\n",
    "- Availble metadata fields:\n",
    "    - Time Granularity\n",
    "    - Geographic Granularity\n",
    "    - Number of Columns\n",
    "    - Number of Rows\n",
    "    - Popularity\n",
    "    - Usability Rating\n",
    "    - Tags\n",
    "    - File Size (bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI \n",
    "import instructor\n",
    "from pydantic import BaseModel, ValidationError, conlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_df = pd.read_csv('eval_dataset_with_task_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7712c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key, model name, and clients\n",
    "load_dotenv()\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "instructor_client = instructor.from_openai(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70660a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataQuery(BaseModel):\n",
    "    queries: conlist(str, min_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4adc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize token usage counters\n",
    "total_tokens_sum = 0\n",
    "prompt_tokens_sum = 0\n",
    "completion_tokens_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metadata_queries(task_queries, metadata):\n",
    "    global total_tokens_sum, prompt_tokens_sum, completion_tokens_sum\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "        Given the analytical task {task_query} and the available dataset metadata: {metadata}, \n",
    "        generate a list with one natural language metadata query for each metadata attribute that expresses the dataset requirement for that attribute needed to implement the given task.\n",
    "\n",
    "        Output a list of metadata queries that are as specific and concise as possible in the specification. Queries should be within but far from the bound of the given metadata attribute value.\n",
    "        So, with the metadata 'Number of Rows' equals to 4836, the corresponding query could be 'The dataset should contain at least 2000 rows', but could not be 'The dataset should contain at least 10000 rows'.\n",
    "\n",
    "        For example, for the task 'Identify patterns in the trading volume of Adobe's stock' and metadata {{\"Tags\": [\"Trading\", \"Forecasting\"], \"Time Granularity\": \"Day\", \"Number of Columns\": 7, \"Number of Rows\": 3563}}, \n",
    "        this could yield: ['The dataset should have the trading tag', 'I need a dataset with data on minimum daily level', 'the dataset should contain roughly 5 columns', 'the dataset should contain at least 2000 rows']\n",
    "        \n",
    "        Only generate a list of metadata queries, excluding any introductory phrases and focusing exclusively on the tasks themselves.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata_queries = []\n",
    "    \n",
    "    # Remove metadata fields with NA values\n",
    "    metadata = {k: v for k, v in metadata.items() if pd.notna(v)}\n",
    "    \n",
    "    for task_query in task_queries:\n",
    "        prompt = prompt_template.format(task_query=task_query, metadata=metadata)\n",
    "        \n",
    "        try:\n",
    "            response, completion = instructor_client.chat.completions.create_with_completion(\n",
    "                model=MODEL,\n",
    "                response_model=MetadataQuery,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed generate metadata queries given an analytical task and the available dataset metadata.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Record token usage details\n",
    "            token_usage = completion.usage\n",
    "            total_tokens_sum += token_usage.total_tokens\n",
    "            prompt_tokens_sum += token_usage.prompt_tokens\n",
    "            completion_tokens_sum += token_usage.completion_tokens\n",
    "            \n",
    "            # Extract and validate the response\n",
    "            extracted_queries = MetadataQuery(**response.__dict__)\n",
    "            metadata_queries.append(extracted_queries.queries)\n",
    "        except ValidationError as ve:\n",
    "            print(f\"ValidationError parsing response: {ve}\")\n",
    "            metadata_queries.append([])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating metadata queries: {e}\")\n",
    "            metadata_queries.append([])\n",
    "    \n",
    "    return metadata_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc48ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metadata_queries(row):\n",
    "    task_queries = ast.literal_eval(row['Task Queries'])\n",
    "    metadata = {\n",
    "        \"Tags\": row['Tags'],\n",
    "        \"Time Granularity\": row['Time Granularity'],\n",
    "        \"Geographic Granularity\": row['Geographic Granularity'],\n",
    "        \"Number of Columns\": row['Number of Columns'],\n",
    "        \"Number of Rows\": row['Number of Rows']\n",
    "    }\n",
    "\n",
    "    return generate_metadata_queries(task_queries, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3072fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame and create the \"Metadata Queries\" column\n",
    "tqdm.pandas(desc=\"Generating metadata queries\")\n",
    "task_keyword_df['Metadata Queries'] = task_keyword_df.progress_apply(process_metadata_queries, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2437d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Tokens Used: {total_tokens_sum}\")\n",
    "print(f\"Prompt Tokens Used: {prompt_tokens_sum}\")\n",
    "print(f\"Completion Tokens Used: {completion_tokens_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_keyword_df.to_csv('eval_dataset_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf312f6",
   "metadata": {},
   "source": [
    "### 3. Evaluation Dataset Split (Database Level): Stratified Sampling by Database Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784597fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea332bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('eval_data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dce2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate table count per database\n",
    "table_counts = df['database_name'].value_counts().to_dict()\n",
    "df['table_count'] = df['database_name'].map(table_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c2896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Categorize databases by table count\n",
    "def categorize_database(size):\n",
    "    if size == 1:\n",
    "        return 'small'\n",
    "    elif 2 <= size <= 10:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "df['db_size_category'] = df['table_count'].apply(categorize_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Stratified split into train+val (60%) and test (40%) on database level\n",
    "# Get unique databases and their categories\n",
    "unique_databases = df[['database_name', 'db_size_category']].drop_duplicates()\n",
    "\n",
    "train_val_dbs, test_dbs = train_test_split(\n",
    "    unique_databases, test_size=0.4, stratify=unique_databases['db_size_category'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b999e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the train+validation (60%) and test (40%) datasets\n",
    "train_val_df = df[df['database_name'].isin(train_val_dbs['database_name'])]\n",
    "test_df = df[df['database_name'].isin(test_dbs['database_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Further split train+validation into training (40%) and validation (20%)\n",
    "train_dbs, val_dbs = train_test_split(\n",
    "    train_val_dbs, test_size=0.333, stratify=train_val_dbs['db_size_category'], random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_val_df[train_val_df['database_name'].isin(train_dbs['database_name'])]\n",
    "val_df = train_val_df[train_val_df['database_name'].isin(val_dbs['database_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save the split data into CSV files\n",
    "train_df.to_csv('eval_data_train.csv', index=False)\n",
    "val_df.to_csv('eval_data_validation.csv', index=False)\n",
    "test_df.to_csv('eval_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab89b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the final splits: # of databases & tables in each split\n",
    "split_summary = {\n",
    "    'Split': ['Training', 'Validation', 'Test'],\n",
    "    'Databases': [\n",
    "        train_df['database_name'].nunique(),\n",
    "        val_df['database_name'].nunique(),\n",
    "        test_df['database_name'].nunique()\n",
    "    ],\n",
    "    'Tables': [\n",
    "        train_df.shape[0],\n",
    "        val_df.shape[0],\n",
    "        test_df.shape[0]\n",
    "    ]\n",
    "}\n",
    "\n",
    "split_summary_df = pd.DataFrame(split_summary)\n",
    "split_summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
